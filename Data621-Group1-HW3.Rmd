---
title: 'Data 621 Homework 3: Boston Crime Rates'
author: "Tommy Jenkins, Violeta Stoyanova, Todd Weigel, Peter Kowalchuk, Eleanor R-Secoquian"
date: "October, 2019"
output:
  html_document:
    number_sections: yes
    theme: paper
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
crimeTrain <- read.csv("crime-training-data_modified.csv")
crimeEval <- read.csv("crime-evaluation-data_modified.csv")
```

# OVERVIEW

In this homework assignment, we will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

## Objective: 

The objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels.

# DATA EXPLORATION

## Data Summary 
```{r echo=FALSE, message=FALSE, warning=FALSE}
crimed1 <- describe(crimeTrain, na.rm = F)
crimed1$na_count <- sapply(crimeTrain, function(y) sum(length(which(is.na(y)))))
crimed1$na_count_perc <- sapply(crimeTrain, function(x) round(sum(is.na(x))/nrow(crimeTrain)*100,1))
```


```{r echo=FALSE,message=FALSE,warning=FALSE}
colsTrain<-ncol(crimeTrain)
colsEval<-ncol(crimeEval)
missingCol<-colnames(crimeTrain)[!(colnames(crimeTrain) %in% colnames(crimeEval))]
```

The dataset consists of two data files: training and evaluation. The training dataset contains `r colsTrain` columns, while the evaluation dataset contains `r colsEval`. The evaluation dataset is missing column `r missingCol` which represend our responce variable and defines whether the crime rate is above the median crime rate (1) or not (0). We will start by exploring the training data set since it will be the one used to generate the regression model.

```{r echo=FALSE,message=FALSE,warning=FALSE}
text<-"a test"
if(all(apply(crimeTrain,2,function(x) is.numeric(x)))==TRUE) {
  text<-"all data is numeric"
} else {
  text<-"not all data is numeric"
}
maxMeanMedianDiff<-round(max(abs(sapply(crimeTrain, median, na.rm = T) - sapply(crimeTrain, mean, na.rm = T))*100/(sapply(crimeTrain, max, na.rm = T)-sapply(crimeTrain, min, na.rm = T))),2)
```

First we see that `r text`. The dataset does contain one dummy variable to identify if the property borders the Charles River (1) or not (0). 

```{r echo=FALSE,message=FALSE,warning=FALSE}
nas<-as.data.frame(sapply(crimeTrain, function(x) sum(is.na(x))))
nasp<-as.data.frame(sapply(crimeTrain, function(x) round(sum(is.na(x))/nrow(crimeTrain)*100,1)))
colnames(nas)<-c("name")
maxna<-max(nas)
maxnaname<-rownames(nas)[nas$name==maxna]
percent<-round(maxna/nrow(crimeTrain)*100,1)
```

An important aspect of any dataset is to determine how much, if any, data is missing. We look at all the variables to see which if any have missing data. We look at the basic descriptive statistics as well as the missing data and their percentages:

```{r echo=FALSE,message=FALSE,warning=FALSE}
kable(crimed1, "html", escape = F) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  column_spec(1, bold = T) %>%
  scroll_box(width = "100%", height = "500px")
sapply(crimeTrain, function(x) round(sum(is.na(x))/nrow(crimeTrain)*100,1))

library(naniar)
vis_miss(crimeTrain)
#gg_miss_upset(crimeTrain)
head(crimeTrain)
```


## Missing and Invalid Data

No missing data was found in the dataset.

With missing data assessed, we can look into the data in more detail. To visualize this we plot histograms for each data. 


```{r echo=FALSE,message=FALSE,warning=FALSE}
attach(crimeTrain[,-1])
ggplot(gather(crimeTrain[,-1]), aes(value)) +
    geom_histogram(bins = 20) +
    facet_wrap(~key, scales = "free_x")
```


## Correlation Plot


# DATA PREPARATION

## Variable Creation / Removal 



# BUILD MODELS

```{r}
m1<-glm(target~.,data=crimeTrain,family="binomial"(link="logit"))
summary(m1)

```
Use Stepwise function to find the best parameters

```{r}
m2 <- step(m1)
summary(m2)
```

This reduces the predictors used in the model to these:
     zn
     nox
     age
     dis
     rad
     tax
     ptRation
     medv

It Removes these predictors:
     indus
     chas
     rm#

The AIC improves marginally (215 vs 218) but we also benefit by having a simpler model less prone to overfitting.

Also, tThe predictors in the model now are all signficant (under 0.05 pr level) and all but one under .01 or very significant. Which is much improved over the prior model




###Create a binned diagnostic plot of residuals vs prediction
There are definite patterns here, which bear investigating.

```{r}
crimeMut <- mutate(crimeTrain, Residuals = residuals(m2), linPred = predict(m2))
grpCrime <- group_by(crimeMut, cut(linPred, breaks=unique(quantile(linPred, (0:25/26)))))

diagCrime <- summarise(grpCrime, Residuals = mean(Residuals), linPred = mean(linPred))
plot(Residuals ~ linPred, data = diagCrime, xlab="Linear Predictor")
```

###Plot leverages.

```{r}
halfnorm(hatvalues(m2))
```

We don't see any strong outliers with the leverage plot.  The points identified (14,18) are essentially in the plot of the line formed, so they are not likely pulling our model in any direction.

###Plot Goodness of fit
```{r}
linPred <- predict(m2)


crimeMut <- mutate(crimeTrain, predProb = predict(m2, type = "response"))
grpCrime <- group_by(crimeMut, cut(linPred, breaks = unique(quantile(linPred, (0:25)/26))))

#hosmer-lemeshow stat
hlDf <- summarise(grpCrime, y= sum(target), pPred=mean(predProb), count = n())

hlDf <- mutate(hlDf, se.fit=sqrt(pPred * (1-(pPred)/count)))


ggplot(hlDf,aes(x=pPred,y=y/count,ymin=y/count-2*se.fit,ymax=y/count+2*se.fit)) +
    geom_point()+geom_linerange(color=grey(0.75))+geom_abline(intercept=0,slope=1) +
    xlab("Predicted Probability") +
    ylab("Observed Proportion")
```

We see that our predictors fall close to the line.  (Note to group, need do adjust the min max line)
								   
			
##BoxCox Model
			   


# SELECT MODELS
## Compare Model Statistics


## Pick the best regression model


## Conclusion




# APPENDIX

**Code used in analysis**

